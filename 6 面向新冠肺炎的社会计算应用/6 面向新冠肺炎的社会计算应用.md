# 面向新冠肺炎的社会计算应用

## 1 任务目标

### 1.1 案例简介

新冠肺炎疫情牵动着我们每一个人的心，在这个案例中，我们将尝试用社会计算的方法对疫情相关的新闻和谣言进行分析，助力疫情信息研究。本次作业为开放性作业，我们提供了疫情期间的社交数据，鼓励同学们从新闻、谣言以及法律文书中分析社会趋势。（提示：运用课上学到的方法，如情感分析、信息抽取、阅读理解等分析数据）

### 1.2 数据说明

﻿https://covid19.thunlp.org/ 提供了与新冠疫情相关的社交数据信息，分别为疫情相关谣言 CSDC-Rumor、疫情相关中文新闻 CSDC-News和疫情相关法律文书 CSDC-Legal。

#### 疫情相关谣言 CSDC-Rumor

这一部分的数据集收集了:

（1）自 2020 年 1 月 22 日开始的[微博不实信息](https://service.account.weibo.com/)数据，包括被认定为不实信息的微博的内容、发布者，以及举报者、审理时间、结果等信息，截至 2020 年 3 月 1 日共 324 条微博原文，31,284 条转发和 7,912 条评论，用于帮助各位研究者分析研究疫情期间的不实信息传播；

（2）自 2020 年 1 月 18 日开始的腾讯谣言验证平台以及丁香园不实信息数据，包括被认定为正确或不实信息的谣言内容、时间以及用以判断是否为谣言的依据等信息，截至 2020 年 3 月 1 日共 507 条谣言数据，其中事实性数据 124 条，数据分布为，负例：420 正例：33 不确定：54。

#### 疫情相关中文新闻 CSDC-News

这一部分的数据集收集了自 2020 年 1 月 1 日开始的新闻数据，包含了新闻的标题、内容、关键词等信息，截至 2020 年 3 月 16 日共收集 148,960 条新闻以及 1,653,086 条对应评论，用于帮助各位研究者分析研究疫情期间的新闻数据。

#### 疫情相关法律文书 CSDC-Legal

该数据为对从 [CAIL](https://arxiv.org/pdf/1807.02478.pdf) 收集的经匿名化的法律文书数据中筛选出的历史上与疫情相关的部分，共 1203 条，每条数据包含了文书标题、案号以及文书全文，供研究者用于进行疫情期间相关法律问题的研究。

### 1.3 参考思路

1. 谣言检测：如何准确快速地识别社交媒体上的谣言是社会计算领域中的一个重要问题，在我们提供的疫情相关谣言数据集上，同学们可以尝试不同的谣言检测方法，比如基于特征[1]、基于神经网络[2, 3]或基于传播模型的方法[4]，综述[5]总结了谣言检测的相关技术。
2. 新闻情感分析：参考我们的情感分析作业，可以通过关键词识别[6]等技术对疫情相关的中文新闻进行情感分析，并找出情感背后蕴含的社会学原因。
3. ﻿[http://weibo.com/n/%E6%B8%85%E5%8D%8E%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86](http://weibo.com/n/清华自然语言处理) 清华自然语言处理实验室微博中给出了一些可视化例子，同学们也可以用统计学和语言学方法对文本进行分析和可视化。

### 1.4 评分标准

本次作业为开放性作业，我们会从

1. 选题的合理性和新颖性
2. 采用方法的合理性和技术含量
3. 作业的完成度和工程量
4. 报告和社会学分析的完整性和深入程度

等方面为作业进行评分。

### 1.5 参考资料

> [1] Information  credibility  on twitter. in Proceedings of WWW, 2011.
>
> [2] Detecting rumors from microblogs with recurrent neural networks. in Proceedings of IJCAI, 2016.
>
> [3] A convolutional approach for misinformation identification. in Proceedings of IJCAI, 2017.
>
> [4] The spread of true and false news online. Science, 2018.
>
> [5] False information on web and social media: A survey. arXiv preprint, 2018.
>
> [6] Characterization of the Affective Norms for English Words by Discrete Emotional Categories. Behavior Research Methods, 2007.

## 2 疫情相关谣言数据分析

​	本次实验提供了疫情相关谣言数据集 CSDC-Rumor，通过对数据集内容分析，选择首先对数据集进行定量统计分析，之后使用聚类实现谣言的语义分析，最后设计一个谣言检测系统。

### 2.1 数据处理

1. 数据格式

   本次实验提供了疫情相关谣言数据集 CSDC-Rumor，收集了微博不实信息数据，与辟谣数据。数据集包含以下内容。

   ```
   rumor
   │  fact.json
   │
   ├─rumor_forward_comment
   │      2020-01-22_K1CaS7Qxd76ol.json
   │      2020-01-23_K1CaS7Q1c768i.json
   ...
   │      2020-03-03_K1CaS8wxh6agf.json
   └─rumor_weibo
           2020-01-22_K1CaS7Qth660h.json
           2020-01-22_K1CaS7Qxd76ol.json
           ...
           2020-03-03_K1CaS8wxh6agf.json
   ```

   **微博不实信息**分别由 `rumor_weibo` 和 `rumor_forward_comment` 中的两个同名 `json` 文件所描述。`rumor_weibo` 中的 `json` 具体字段如下：

   - `rumorCode`: 该条谣言的唯一编码，可以通过该编码直接访问该谣言举报页面。
   - `title`: 该条谣言被举报的标题内容。
   - `informerName`: 举报者微博名称。
   - `informerUrl`: 举报者微博链接。
   - `rumormongerName`: 发布谣言者的微博名称。
   - `rumormongerUr`: 发布谣言者的微博链接。
   - `rumorText`: 谣言内容。
   - `visitTimes`: 该谣言被访问次数。
   - `result`: 该谣言审查结果。
   - `publishTime`: 该谣言被举报时间。
   - `related_url`: 与该谣言相关的证据、规定等链接。

   `rumor_forward_comment` 中的 `json` 具体字段如下：

   - `uid`: 发表用户 ID。
   - `text`: 评论或转发附言文字。
   - `date`: 发布时间。
   - `comment_or_forward`: 二值，要么是 `comment`，要么是 `forward`，表示该条信息是评论还是转发附言。

   **腾讯与丁香园不实信息**内容格式为：

   - `date`: 时间
   - `explain`: 谣言类型
   - `tag`: 谣言标签
   - `abstract`: 用以验证谣言的内容
   - `rumor`: 谣言

2. 数据预处理

   通过 `json.load()` 分别提取谣言微博数据 `weibo_data` 与 谣言评论转发数据 `forward_comment_data` ，然后将其转换为 DataFrame 格式。其中二者同名的文件，微博文章与微博评论转发相对应，在处理rumor_forward_comment文件夹中的数据时，添加rumorCode以便后续匹配。

   ```py
   # 文件路径
   weibo_dir = 'data/rumor/rumor_weibo'
   forward_comment_dir = 'data/rumor/rumor_forward_comment'
   
   # 初始化数据列表
   weibo_data = []
   forward_comment_data = []
   
   # 处理rumor_weibo文件夹中的数据
   for filename in os.listdir(weibo_dir):
       if filename.endswith('.json'):
           filepath = os.path.join(weibo_dir, filename)
           with open(filepath, 'r', encoding='utf-8') as file:
               data = json.load(file)
               weibo_data.append(data)
   
   # 处理rumor_forward_comment文件夹中的数据
   for filename in os.listdir(forward_comment_dir):
       if filename.endswith('.json'):
           filepath = os.path.join(forward_comment_dir, filename)
           with open(filepath, 'r', encoding='utf-8') as file:
               data = json.load(file)
               # 提取rumorCode
               rumor_code = filename.split('_')[1].split('.')[0]
               for comment in data:
                   comment['rumorCode'] = rumor_code  # 添加rumorCode以便后续匹配
                   forward_comment_data.append(comment)
   
   # 转换为DataFrame
   weibo_df = pd.DataFrame(weibo_data)
   forward_comment_df = pd.DataFrame(forward_comment_data)
   ```

### 2.2 谣言的定量统计分析

​	本节通过定量统计分析，使得对疫情谣言微博数据分布有具体的了解。

1. 谣言被访问次数统计

   统计 `weibo_df['visitTimes']` 访问次数分布，并绘制对应的柱状图，结果如下。

   ![image-20240708173948337](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240708173948337.png)

   通过微博访问次数得知，大部分疫情谣言微博访问次数都在500次以内，其中10-50次的占比最多。但是也存在访问大于5000次的谣言微博，属于造成严重影响，在法律上达到 “情节严重” 的地步。

2. 造谣者与举报者出现次数统计

   通过统计 `weibo_df['rumormongerName']` 与 `weibo_df['informerName']` 得到每个发布谣言者发布谣言的数量与每个举报者举报谣言的数量，结果如下。

   ![image-20240708175825599](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240708175825599.png)

   可以看到，发布造谣者发布谣言数量并不是集中在某几个人上，而是比较平均，发布谣言最多的账号发布了3篇谣言微博。而 Top10 举报者每人至少举报了而3篇谣言文章，其中举报者 风fun道 举报造谣微博数量显著高于其他用户，达到37篇。

   根据以上数据，可以重点观众举报谣言数量多的账号，方便对谣言的侦测。

3. 谣言转发评论量分布统计

   通过统计谣言转发量与评论量分布，得到以下分布图像。

   ![image-20240708180452385](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240708180452385.png)

   可以看到，大部分谣言微博的评论与转发数量都在10次以内，评论量最多不超过500个，而转发量最多达到了10000以上。根据网络管理法，谣言转发量大于500，属于 “情节严重” 情况。

### 2.3 谣言语义分析

1. 谣言文本聚类分析

   本部分通过对微博谣言文本进行数据预处理，分词后，进行聚类分析，看看微博谣言集中在哪些方面。

   - 数据预处理

     首先对谣言数据文本进行清洗，去除缺省值，与 `<>` 括起来的链接内容。

     ```py
     # 去除缺失值
     weibo_df_rumorText = weibo_df.dropna(subset=['rumorText'])
     
     def clean_text(text):
         # 定义正则表达式模式，匹配 <>
         pattern = re.compile(r'<.*?>')
         # 使用 sub 方法删除匹配的部分
         cleaned_text = re.sub(pattern, '', text)
         return cleaned_text
     
     weibo_df_rumorText['rumorText'] = weibo_df_rumorText['rumorText'].apply(clean_text)
     ```

     然后加载中文停用词，停用词采用 [cn_stopwords](https://github.com/goto456/stopwords/blob/master/cn_stopwords.txt) ，利用 `jieba` 实现对数据的分词处理，并进行文本向量化。

     ```py
     # 加载停用词文件
     with open('cn_stopwords.txt', encoding='utf-8') as f:
         stopwords = set(f.read().strip().split('\n'))
     
     # 分词和去停用词函数
     def preprocess_text(text):
         words = jieba.lcut(text)
         words = [word for word in words if word not in stopwords and len(word) > 1]
         return ' '.join(words)
     
     # 应用到数据集
     weibo_df_rumorText['processed_text'] = weibo_df_rumorText['rumorText'].apply(preprocess_text)
     
     # 文本向量化
     vectorizer = TfidfVectorizer(max_features=10000)
     X_tfidf = vectorizer.fit_transform(weibo_df_rumorText['processed_text'])
     ```

   - 确定最佳聚类

     通过使用肘部法，确定最佳聚类。

     肘部法（Elbow Method）是一种用于确定聚类分析中最佳聚类数目的方法。它基于误差平方和（SSE，Sum of Squared Errors）与聚类数目之间的关系。SSE是聚类中所有数据点到其所属聚类中心的欧氏距离的平方和，它反映了聚类的效果：SSE越小，表示聚类效果越好。

     ```py
     # 使用肘部法确定最佳聚类数量
     def plot_elbow_method(X):
         sse = []
         for k in range(1, 21):
             kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
             kmeans.fit(X)
             sse.append(kmeans.inertia_)
         plt.plot(range(1, 21), sse, marker='o')
         plt.xlabel('Number of clusters')
         plt.ylabel('SSE')
         plt.title('Elbow Method For Optimal k')
         plt.show()
     
     # 绘制肘部法图
     plot_elbow_method(X_tfidf)
     ```

     肘部法通过寻找“肘部”来确定最佳聚类数量，即在曲线上寻找一个点，这个点之后SSE下降的速率明显减慢，这个点就像胳膊的肘部一样，因此得名“肘部法”。这个点通常被认为是最佳的聚类数目。

     ![image-20240708184711285](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240708184711285.png)

     由上图，确定肘部的聚类值为 11，绘制相应散点图，结果如下。

     ![image-20240708185243978](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240708185243978.png)

     可以看出，大部分谣言微博进行了较好的聚类，3号与4号；也有的分布比较广，没有十分好的聚类，如5号，8号。

   - 聚类结果

     为了明显的表示每个类都聚类了哪些谣言，将每个类都绘制一副云图，结果如下。

     ![image-20240708185416278](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240708185416278.png)

     打印出一些聚类较好的谣言微博内容，结果如下。

     ```
     Cluster 2:
     13    #封城日记#我的城市是湖北最后一个被封城的城市，金庸笔下的襄阳，郭靖黄蓉守城战死的地方。至此...
     17    #襄阳封城#最后一座城被封，自此，湖北封省。在这大年三十夜。春晚正在说，为中国七十年成就喝彩...
     18    #襄阳封城#过会襄阳也要封城了，到O25号0点，湖北省全省封省 ，希望襄阳的疑似患者能尽快确...
     19    截止2020年1月25日00:00时❌湖北最后一个城市襄阳，整个湖北所有城市封城完毕，无公交...
     21    #襄阳封城# 湖北省最后的一个城市于2020年1月25日零点零时零分正式封城至此，湖北省，封...
     Name: rumorText, dtype: object
     
     Cluster 3:
     212    希望就在眼前…我期待着这一天早点到来。一刻都等不及了…西安学校开学时间：高三，初三，3月2日...
     213    学校开学时间：高三，初三，3月2日开学，高一，高二，初一，初二，3月9日开学，小学4一6年级...
     224    #长春爆料#【马上要解放了？真的假的?】长春学校开学时间：高三，初三，3月2日开学，高一，高...
     225    有谁知道这是真的吗？？我们这些被困湖北的外地人到底什么时候才能回家啊！（市政府会议精神，3月...
     226    山东邹平市公布小道消息公布~:学校开学时间：高三，初三，3月2日开学，高一，高二，初一，初二...
     Name: rumorText, dtype: object
     ```

2. 谣言审查结果聚类分析

   通过谣言文本内容聚类可能对谣言内容分析表示还没那么好，于是选择对谣言审查结果聚类分析。

   - 确定最佳聚类

     使用肘部图，确定最佳聚类。

     ![image-20240708192216975](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240708192216975.png)

     由以上肘部图，可以确定两个肘部，一个是在聚类为5时，一个是聚类位20时，我选择20进行聚类。

     聚20类得到的散点图如下。

     ![image-20240708192500231](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240708192500231.png)

     可以看到，大部分得到了很好聚类，但是第7类和17类没有很好聚类。

   - 聚类结果

     为了明显的表示每个类都聚类了哪些谣言审查结果，将每个类都绘制一副云图，结果如下。

     ![image-20240708192609138](https://raw.githubusercontent.com/ZzDarker/figure/main/img/image-20240708192609138.png)

     打印出一些聚类较好的谣言审查结果内容，结果如下。

     ```
     Cluster 4:
     52    从武汉撤回来的日本人，迎接他们的是每个人一台救护车，206人=206台救护车
     53    从武汉撤回来的日本人，迎接他们的是每个人一台救护车，206人=206台救护车
     54    从武汉撤回来的日本人，迎接他们的是每个人一台救护车，206人=206台救护车
     55    从武汉撤回来的日本人，迎接他们的是每个人一台救护车，206人=206台救护车
     56    从武汉撤回来的日本人，迎接他们的是每个人一台救护车，206人=206台救护车
     Name: result, dtype: object
     
     Cluster 10:
     214    所有被啃噬、机化的肺组织都不会再恢复了，愈后会形成无任何肺功能的瘢痕组织
     215    所有被啃噬、机化的肺组织都不会再恢复了，愈后会形成无任何肺功能的瘢痕组织
     216    所有被啃噬、机化的肺组织都不会再恢复了，愈后会形成无任何肺功能的瘢痕组织
     217    所有被啃噬、机化的肺组织都不会再恢复了，愈后会形成无任何肺功能的瘢痕组织
     218    所有被啃噬、机化的肺组织都不会再恢复了，愈后会形成无任何肺功能的瘢痕组织
     Name: result, dtype: object
     
     Cluster 15:
     7           在福州，里面坐的是周杰伦
     8            周杰伦去福州自备隔离仓
     9            周杰伦去福州自备隔离仓
     10    周杰伦福州演唱会，给自己整了个隔离舱
     12    周杰伦福州演唱会，给自己整了个隔离舱
     Name: result, dtype: object
     ```

### 2.4 谣言检测

​	本次谣言检测，选择采用已经辟谣的数据集 `fact.json` 中的辟谣谣言与真实谣言进行对比相似度，选择与谣言微博相似度最高的辟谣文章，作为谣言检测的依据。

1. 加载微博谣言数据和辟谣数据集

   ```py
   # 定义一个空的列表来存储每个 JSON 对象
   fact_data = []
   
   # 逐行读取 JSON 文件
   with open('data/rumor/fact.json', 'r', encoding='utf-8') as f:
       for line in f:
           fact_data.append(json.loads(line.strip()))
   
   # 创建辟谣数据的 DataFrame
   fact_df = pd.DataFrame(fact_data)
   fact_df = fact_df.dropna(subset=['title'])
   ```

2. 使用预训练的语言模型将微博谣言和辟谣标题编码成嵌入向量

   本次实验使用 `bert-base-chinese` 作为预训练模型，进行模型训练。并采用SimCSE模型，通过对比学习来提升句子语义的表示和相似度度量。

   ```py
   # 加载SimCSE模型
   model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
   # 加载到 GPU
   model.to('cuda')
   
   # 加载预训练的NER模型
   ner_pipeline = pipeline('ner', model='bert-base-chinese', aggregation_strategy="simple", device=0)
   ```

3. 计算相似度

   计算相似度选择采用了SimCSE模型的句子嵌入和命名实体相似度计算综合相似度。

   - `extract_entities`函数使用NER模型提取文本中的命名实体。

     ```py
     # 提取命名实体
     def extract_entities(text):
         entities = ner_pipeline(text)
         return {entity['word'] for entity in entities}
     ```

   - `entity_similarity`函数计算两个文本之间的命名实体相似度。

     ```py
     # 计算命名实体相似度
     def entity_similarity(text1, text2):
         entities1 = extract_entities(text1)
         entities2 = extract_entities(text2)
         if not entities1 or not entities2:
             return 0.0
         intersection = entities1.intersection(entities2)
         union = entities1.union(entities2)
         return len(intersection) / len(union)
     ```

   - `combined_similarity`函数结合SimCSE模型的句子嵌入和命名实体相似度计算综合相似度。

     ```py
     # 结合句子嵌入相似度和实体相似度
     def combined_similarity(text1, text2):
         embed_sim = util.pytorch_cos_sim(model.encode([text1]), model.encode([text2])).item()
         entity_sim = entity_similarity(text1, text2)
         return 0.5 * embed_sim + 0.5 * entity_sim
     ```

4. 实现谣言检测

   通过对比相似度，实现谣言检测机制。

   ```py
   def debunk_rumor(input_rumor):
       # 计算输入谣言与所有辟谣标题的相似度
       similarity_scores = []
       for fact_text in fact_df['title']:
           similarity_scores.append(combined_similarity(input_rumor, fact_text))
       
       # 找到最相似的辟谣标题
       most_similar_index = np.argmax(similarity_scores)
       most_similar_fact = fact_df.iloc[most_similar_index]
       
       # 输出辟谣判断及依据
       print("微博谣言:", input_rumor)
       print(f"辟谣判断：{most_similar_fact['explain']}")
       print(f"辟谣依据：{most_similar_fact['title']}")
   
   weibo_rumor = "据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺——一种犬科动物携带的病毒，然后传给附近的狗，狗传狗，狗传人。狗不生病，人生病。人生病后又传给狗，循环传染。"
   debunk_rumor(weibo_rumor)
   ```

   输出结果如下：

   ```
   微博谣言: 据最新研究发现，此次新型肺炎病毒传播途径是华南海鲜市场进口的豺——一种犬科动物携带的病毒，然后传给附近的狗，狗传狗，狗传人。狗不生病，人生病。人生病后又传给狗，循环传染。
   辟谣判断：尚无定论
   辟谣依据：狗能感染新型冠状病毒
   ```

   成功找到辟谣依据，给出辟谣判断。

## 3 疫情相关中文新闻数据分析

### 3.1 数据加载



### 3.2 新闻内容数据分析



### 3.3 新闻评论数据分析