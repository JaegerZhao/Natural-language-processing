(jupyter) D:\study\学习资料\驭风计划\4 自然语言处理\Natural language processing\1 Word2VecTransE\Word2Vec>python word2vec.py                             
2024-04-25 11:04:58,460 : INFO : collecting all words and their counts
2024-04-25 11:04:58,462 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types
2024-04-25 11:05:03,104 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences
2024-04-25 11:05:03,104 : INFO : Creating a fresh vocabulary
2024-04-25 11:05:03,318 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 47134 unique words (18.57% of original 253854, drops 206720)', 'datetime': '2024-04-25T11:05:03.297659', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'prepare_vocab'}
2024-04-25 11:05:03,318 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 16561031 word corpus (97.39% of original 17005207, drops 444176)', 'datetime': '2024-04-25T11:05:03.318747', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'prepare_vocab'}
2024-04-25 11:05:03,541 : INFO : deleting the raw counts dictionary of 253854 items
2024-04-25 11:05:03,549 : INFO : sample=0.001 downsamples 38 most-common words
2024-04-25 11:05:03,550 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 12333563.370024087 word corpus (74.5%% of prior 16561031)', 'datetime': '2024-04-25T11:05:03.550877', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'prepare_vocab'}
2024-04-25 11:05:03,872 : INFO : estimated required memory for 47134 words and 200 dimensions: 98981400 bytes
2024-04-25 11:05:03,873 : INFO : resetting layer weights
2024-04-25 11:05:03,922 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-04-25T11:05:03.922201', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'build_vocab'}
2024-04-25 11:05:03,923 : INFO : Word2Vec lifecycle event {'msg': 'training model with 8 workers on 47134 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 
window=10 shrink_windows=True', 'datetime': '2024-04-25T11:05:03.923203', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'train'}
2024-04-25 11:05:04,938 : INFO : EPOCH 0 - PROGRESS: at 7.52% examples, 911313 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:05,943 : INFO : EPOCH 0 - PROGRESS: at 15.87% examples, 965489 words/s, in_qsize 0, out_qsize 1
2024-04-25 11:05:06,953 : INFO : EPOCH 0 - PROGRESS: at 23.87% examples, 971255 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:07,955 : INFO : EPOCH 0 - PROGRESS: at 31.51% examples, 966069 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:08,966 : INFO : EPOCH 0 - PROGRESS: at 36.51% examples, 896186 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:09,971 : INFO : EPOCH 0 - PROGRESS: at 42.80% examples, 876047 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:10,973 : INFO : EPOCH 0 - PROGRESS: at 50.97% examples, 895024 words/s, in_qsize 0, out_qsize 1
2024-04-25 11:05:11,978 : INFO : EPOCH 0 - PROGRESS: at 58.79% examples, 903516 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:12,984 : INFO : EPOCH 0 - PROGRESS: at 67.02% examples, 915474 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:13,992 : INFO : EPOCH 0 - PROGRESS: at 74.02% examples, 910095 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:14,998 : INFO : EPOCH 0 - PROGRESS: at 80.01% examples, 892460 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:16,006 : INFO : EPOCH 0 - PROGRESS: at 87.36% examples, 892800 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:17,012 : INFO : EPOCH 0 - PROGRESS: at 94.71% examples, 893200 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:17,704 : INFO : EPOCH 0: training on 17005207 raw words (12334710 effective words) took 13.8s, 895417 effective words/s
2024-04-25 11:05:18,712 : INFO : EPOCH 1 - PROGRESS: at 6.76% examples, 822104 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:19,716 : INFO : EPOCH 1 - PROGRESS: at 13.87% examples, 846230 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:20,718 : INFO : EPOCH 1 - PROGRESS: at 21.11% examples, 860506 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:21,726 : INFO : EPOCH 1 - PROGRESS: at 28.45% examples, 872855 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:22,732 : INFO : EPOCH 1 - PROGRESS: at 35.98% examples, 885264 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:23,737 : INFO : EPOCH 1 - PROGRESS: at 43.45% examples, 890786 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:24,742 : INFO : EPOCH 1 - PROGRESS: at 51.38% examples, 903254 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:25,744 : INFO : EPOCH 1 - PROGRESS: at 58.50% examples, 900232 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:26,748 : INFO : EPOCH 1 - PROGRESS: at 66.55% examples, 910559 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:27,749 : INFO : EPOCH 1 - PROGRESS: at 73.90% examples, 910506 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:28,754 : INFO : EPOCH 1 - PROGRESS: at 81.60% examples, 911912 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:29,754 : INFO : EPOCH 1 - PROGRESS: at 88.54% examples, 907109 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:30,757 : INFO : EPOCH 1 - PROGRESS: at 96.53% examples, 912656 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:31,170 : INFO : EPOCH 1: training on 17005207 raw words (12334174 effective words) took 13.5s, 916150 effective words/s
2024-04-25 11:05:32,184 : INFO : EPOCH 2 - PROGRESS: at 8.23% examples, 996634 words/s, in_qsize 0, out_qsize 1
2024-04-25 11:05:33,190 : INFO : EPOCH 2 - PROGRESS: at 16.46% examples, 1000725 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:34,192 : INFO : EPOCH 2 - PROGRESS: at 24.63% examples, 1003909 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:35,192 : INFO : EPOCH 2 - PROGRESS: at 32.63% examples, 1002504 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:36,203 : INFO : EPOCH 2 - PROGRESS: at 40.68% examples, 999723 words/s, in_qsize 0, out_qsize 1
2024-04-25 11:05:37,265 : INFO : EPOCH 2 - PROGRESS: at 46.68% examples, 947593 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:38,269 : INFO : EPOCH 2 - PROGRESS: at 52.38% examples, 912770 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:39,269 : INFO : EPOCH 2 - PROGRESS: at 59.32% examples, 906289 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:40,274 : INFO : EPOCH 2 - PROGRESS: at 65.26% examples, 886948 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:41,279 : INFO : EPOCH 2 - PROGRESS: at 72.66% examples, 889575 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:42,287 : INFO : EPOCH 2 - PROGRESS: at 79.66% examples, 885020 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:43,299 : INFO : EPOCH 2 - PROGRESS: at 84.95% examples, 864865 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:44,303 : INFO : EPOCH 2 - PROGRESS: at 91.48% examples, 859968 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:45,305 : INFO : EPOCH 2 - PROGRESS: at 97.47% examples, 850957 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:45,882 : INFO : EPOCH 2: training on 17005207 raw words (12335691 effective words) took 14.7s, 838664 effective words/s
2024-04-25 11:05:46,930 : INFO : EPOCH 3 - PROGRESS: at 6.53% examples, 763748 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:47,943 : INFO : EPOCH 3 - PROGRESS: at 13.76% examples, 818970 words/s, in_qsize 0, out_qsize 1
2024-04-25 11:05:48,948 : INFO : EPOCH 3 - PROGRESS: at 21.81% examples, 874648 words/s, in_qsize 0, out_qsize 1
2024-04-25 11:05:49,950 : INFO : EPOCH 3 - PROGRESS: at 29.92% examples, 908198 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:50,951 : INFO : EPOCH 3 - PROGRESS: at 38.10% examples, 929869 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:51,961 : INFO : EPOCH 3 - PROGRESS: at 46.27% examples, 941733 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:52,972 : INFO : EPOCH 3 - PROGRESS: at 54.56% examples, 952367 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:53,976 : INFO : EPOCH 3 - PROGRESS: at 62.79% examples, 959923 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:54,978 : INFO : EPOCH 3 - PROGRESS: at 71.02% examples, 966208 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:55,979 : INFO : EPOCH 3 - PROGRESS: at 79.37% examples, 970850 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:56,983 : INFO : EPOCH 3 - PROGRESS: at 87.54% examples, 973444 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:57,986 : INFO : EPOCH 3 - PROGRESS: at 95.71% examples, 976013 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:05:58,495 : INFO : EPOCH 3: training on 17005207 raw words (12333709 effective words) took 12.6s, 978060 effective words/s
2024-04-25 11:05:59,498 : INFO : EPOCH 4 - PROGRESS: at 8.17% examples, 1000053 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:00,501 : INFO : EPOCH 4 - PROGRESS: at 16.28% examples, 996534 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:01,510 : INFO : EPOCH 4 - PROGRESS: at 24.46% examples, 998913 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:02,514 : INFO : EPOCH 4 - PROGRESS: at 32.57% examples, 1001307 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:03,518 : INFO : EPOCH 4 - PROGRESS: at 40.68% examples, 1001504 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:04,522 : INFO : EPOCH 4 - PROGRESS: at 48.85% examples, 1002910 words/s, in_qsize 0, out_qsize 1
2024-04-25 11:06:05,528 : INFO : EPOCH 4 - PROGRESS: at 56.97% examples, 1002229 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:06,537 : INFO : EPOCH 4 - PROGRESS: at 65.20% examples, 1002880 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:07,545 : INFO : EPOCH 4 - PROGRESS: at 73.37% examples, 1003320 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:08,550 : INFO : EPOCH 4 - PROGRESS: at 81.60% examples, 1001925 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:09,551 : INFO : EPOCH 4 - PROGRESS: at 89.65% examples, 1001100 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:10,556 : INFO : EPOCH 4 - PROGRESS: at 97.88% examples, 1001123 words/s, in_qsize 0, out_qsize 0
2024-04-25 11:06:10,798 : INFO : EPOCH 4: training on 17005207 raw words (12331272 effective words) took 12.3s, 1002561 effective words/s
2024-04-25 11:06:10,798 : INFO : Word2Vec lifecycle event {'msg': 'training on 85026035 raw words (61669556 effective words) took 66.9s, 922157 effective words/s', 'datetime': '2024-04-25T11:06:10.798335', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'train'}
2024-04-25 11:06:10,799 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=47134, vector_size=200, alpha=0.025>', 'datetime': '2024-04-25T11:06:10.799334', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'created'}
2024-04-25 11:06:10,800 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'word2vec_gensim', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-04-25T11:06:10.800330', 'gensim': '4.3.2', 'python': '3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19043-SP0', 'event': 'saving'}
2024-04-25 11:06:10,812 : INFO : not storing attribute cum_table
2024-04-25 11:06:10,929 : INFO : saved word2vec_gensim
2024-04-25 11:06:10,958 : INFO : storing vocabulary in vocabulary
2024-04-25 11:06:11,009 : INFO : storing 47134x200 projection weights into word2vec_org
Total time: 77 s

(jupyter) D:\study\学习资料\驭风计划\4 自然语言处理\Natural language processing\1 Word2VecTransE\Word2Vec>cd ..  

(jupyter) D:\study\学习资料\驭风计划\4 自然语言处理\Natural language processing\1 Word2VecTransE>cd ..

(jupyter) D:\study\学习资料\驭风计划\4 自然语言处理\Natural language processing>cd 1 Word2VecTransE\TransE

(jupyter) D:\study\学习资料\驭风计划\4 自然语言处理\Natural language processing\1 Word2VecTransE\TransE>python TransE.py
D:\software\anaconda3\envs\jupyter\Lib\site-packages\torch\nn\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
Epoch 1 | loss: 145347.006592
Epoch 2 | loss: 97335.424683
Epoch 3 | loss: 72861.278076
Epoch 4 | loss: 57703.964661
Epoch 5 | loss: 47030.031555
Epoch 6 | loss: 39534.021027
Epoch 7 | loss: 33701.517914
Epoch 8 | loss: 29204.247955
Epoch 9 | loss: 25657.601776
Epoch 10 | loss: 22815.791245
Epoch 11 | loss: 20551.937256
Epoch 12 | loss: 18535.730179
Epoch 13 | loss: 16937.606644
Epoch 14 | loss: 15548.305634
Epoch 15 | loss: 14194.062805
Epoch 16 | loss: 13302.658882
Epoch 17 | loss: 12304.918739
Epoch 18 | loss: 11500.714432
Epoch 19 | loss: 10638.875572
Epoch 20 | loss: 10065.777740
Epoch 21 | loss: 9439.877357
Epoch 22 | loss: 8925.135696
Epoch 23 | loss: 8440.957588
Epoch 24 | loss: 7972.531799
Epoch 25 | loss: 7567.640202
Epoch 26 | loss: 7073.779320
Epoch 27 | loss: 6767.861553
Epoch 28 | loss: 6539.013435
Epoch 29 | loss: 6146.200573
Epoch 30 | loss: 5933.153255
Epoch 31 | loss: 5684.782005
Epoch 32 | loss: 5481.362522
Epoch 33 | loss: 5180.982121
Epoch 34 | loss: 4966.714581
Epoch 35 | loss: 4905.094624
Epoch 36 | loss: 4633.501320
Epoch 37 | loss: 4475.392487
Epoch 38 | loss: 4363.053793
Epoch 39 | loss: 4205.629736
Epoch 40 | loss: 4063.885675
Epoch 41 | loss: 3872.228849
Epoch 42 | loss: 3784.661869
Epoch 43 | loss: 3772.204174
Epoch 44 | loss: 3541.447512
Epoch 45 | loss: 3444.607849
Epoch 46 | loss: 3490.825455
Epoch 47 | loss: 3274.204304
Epoch 48 | loss: 3262.753597
Epoch 49 | loss: 3165.682636
Epoch 50 | loss: 3195.219149
Epoch 51 | loss: 3016.931273
Epoch 52 | loss: 2968.049559
Epoch 53 | loss: 2908.047073
Epoch 54 | loss: 2824.532358
Epoch 55 | loss: 2736.795557
Epoch 56 | loss: 2698.738964
Epoch 57 | loss: 2646.264132
Epoch 58 | loss: 2569.739357
Epoch 59 | loss: 2575.101753
Epoch 60 | loss: 2488.206936
Epoch 61 | loss: 2450.686169
Epoch 62 | loss: 2442.746155
Epoch 63 | loss: 2334.968694
Epoch 64 | loss: 2256.705210
Epoch 65 | loss: 2282.284286
Epoch 66 | loss: 2220.966212
Epoch 67 | loss: 2187.133240
Epoch 68 | loss: 2108.195169
Epoch 69 | loss: 2160.212297
Epoch 70 | loss: 2075.789186
Epoch 71 | loss: 2095.767881
Epoch 72 | loss: 1976.792145
Epoch 73 | loss: 1994.586889
Epoch 74 | loss: 1931.886928
Epoch 75 | loss: 1909.305505
Epoch 76 | loss: 1886.725221
Epoch 77 | loss: 1874.180543
Epoch 78 | loss: 1816.217962
Epoch 79 | loss: 1788.045438
Epoch 80 | loss: 1821.554689
Epoch 81 | loss: 1701.208815
Epoch 82 | loss: 1768.621033
Epoch 83 | loss: 1711.707770
Epoch 84 | loss: 1659.133942
Epoch 85 | loss: 1631.992958
Epoch 86 | loss: 1672.001594
Epoch 87 | loss: 1596.892339
Epoch 88 | loss: 1595.880460
Epoch 89 | loss: 1568.144202
Epoch 90 | loss: 1584.218681
Epoch 91 | loss: 1561.090528
Epoch 92 | loss: 1539.754370
Epoch 93 | loss: 1549.933521
Epoch 94 | loss: 1521.429277
Epoch 95 | loss: 1488.630775
Epoch 96 | loss: 1457.744335
Epoch 97 | loss: 1457.046740
Epoch 98 | loss: 1435.964871
Epoch 99 | loss: 1352.940959
Epoch 100 | loss: 1383.982081
Finish Training

(jupyter) D:\study\学习资料\驭风计划\4 自然语言处理\Natural language processing\1 Word2VecTransE\TransE>
(jupyter) D:\study\学习资料\驭风计划\4 自然语言处理\Natural language processing\1 Word2VecTransE\TransE>python TransE.py
Epoch 1 | loss: 46.278254
Epoch 2 | loss: 42.770077
Epoch 3 | loss: 40.945018
Epoch 4 | loss: 39.628672
Epoch 5 | loss: 38.416960
Epoch 6 | loss: 37.326870
Epoch 7 | loss: 36.333397
Epoch 8 | loss: 35.389013
Epoch 9 | loss: 34.593586
Epoch 10 | loss: 33.736216
Epoch 11 | loss: 33.108364
Epoch 12 | loss: 32.286392
Epoch 13 | loss: 31.506627
Epoch 14 | loss: 30.783794
Epoch 15 | loss: 30.148341
Epoch 16 | loss: 29.523119
Epoch 17 | loss: 28.889386
Epoch 18 | loss: 28.346939
Epoch 19 | loss: 27.789625
Epoch 20 | loss: 27.134031
Epoch 21 | loss: 26.455894
Epoch 22 | loss: 25.889345
Epoch 23 | loss: 25.196761
Epoch 24 | loss: 24.780363
Epoch 25 | loss: 24.054965
Epoch 26 | loss: 23.564248
Epoch 27 | loss: 22.845185
Epoch 28 | loss: 22.331272
Epoch 29 | loss: 21.783523
Epoch 30 | loss: 21.243281
Epoch 31 | loss: 20.614033
Epoch 32 | loss: 20.171842
Epoch 33 | loss: 19.572727
Epoch 34 | loss: 19.066367
Epoch 35 | loss: 18.590066
Epoch 36 | loss: 18.191601
Epoch 37 | loss: 17.470898
Epoch 38 | loss: 16.857897
Epoch 39 | loss: 16.389843
Epoch 40 | loss: 15.753218
Epoch 41 | loss: 15.236621
Epoch 42 | loss: 14.917521
Epoch 43 | loss: 14.439256
Epoch 44 | loss: 13.713562
Epoch 45 | loss: 13.249100
Epoch 46 | loss: 12.829056
Epoch 47 | loss: 12.287933
Epoch 48 | loss: 11.829535
Epoch 49 | loss: 11.349364
Epoch 50 | loss: 10.856332
Epoch 51 | loss: 10.513939
Epoch 52 | loss: 10.044648
Epoch 53 | loss: 9.549640
Epoch 54 | loss: 8.995086
Epoch 55 | loss: 8.482237
Epoch 56 | loss: 8.091331
Epoch 57 | loss: 7.632276
Epoch 58 | loss: 7.216859
Epoch 59 | loss: 6.786360
Epoch 60 | loss: 6.302615
Epoch 61 | loss: 5.839602
Epoch 62 | loss: 5.391880
Epoch 63 | loss: 4.862805
Epoch 64 | loss: 4.479973
Epoch 65 | loss: 3.891138
Epoch 66 | loss: 3.610520
Epoch 67 | loss: 3.220157
Epoch 68 | loss: 2.685282
Epoch 69 | loss: 2.163546
Epoch 70 | loss: 1.685094
Epoch 71 | loss: 1.440521
Epoch 72 | loss: 0.967022
Epoch 73 | loss: 0.697158
Epoch 74 | loss: 0.526670
Epoch 75 | loss: 0.302562
Epoch 76 | loss: 0.219380
Epoch 77 | loss: 0.178123
Epoch 78 | loss: 0.103355
Epoch 79 | loss: 0.085977
Epoch 80 | loss: 0.086684
Epoch 81 | loss: 0.066870
Epoch 82 | loss: 0.047100
Epoch 83 | loss: 0.066472
Epoch 84 | loss: 0.064257
Epoch 85 | loss: 0.038392
Epoch 86 | loss: 0.049306
Epoch 87 | loss: 0.030743
Epoch 88 | loss: 0.040390
Epoch 89 | loss: 0.015597
Epoch 90 | loss: 0.027130
Epoch 91 | loss: 0.010972
Epoch 92 | loss: 0.024186
Epoch 93 | loss: 0.013941
Epoch 94 | loss: 0.018634
Epoch 95 | loss: 0.036049
Epoch 96 | loss: 0.016536
Epoch 97 | loss: 0.011090
Epoch 98 | loss: 0.007512
Epoch 99 | loss: 0.015288
Epoch 100 | loss: 0.016592
Finish Training   